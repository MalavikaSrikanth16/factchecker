# deployment_type: "local" to use locally hosted model, "inference_client" to use Hugging Face InferenceClient
deployment_type: "inference_client"

# Model name 
# For inference_client: you can use larger models available on HF Inference API (Tested with "openai/gpt-oss-120b")
#                       NOTE: "HuggingFaceTB/SmolLM2-1.7B-Instruct" is NOT available via HF Inference API
# For local: use small models that can be locally hosted (Tested with "HuggingFaceTB/SmolLM2-1.7B-Instruct")
model: "openai/gpt-oss-120b"

system_prompt_path: "prompts/system_prompt.txt"
user_prompt_path: "prompts/user_prompt.txt"
temperature: 0

# wiki_agentic_rag: if true, requires deployment_type to be "inference_client"
wiki_agentic_rag: true
# izzyviz will only run if deployment_type is "local"
izzyviz: false